"""lstm
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu
"""
!pip install torch
!pip install torchtext
!python -m spacy download en

import torch
import torchtext
from torch import nn, optim
from torchtext import data, datasets

print('GPU:', torch.cuda.is_available())
print(torchtext.__version__)

"""
设置PyTorch随机数生成器的种子为123，以确保在相同的种子下产生相同的随机数序列。
"""
torch.manual_seed(123)
"""
定义文本数据预处理的类。其中的tokenize参数是指用于将文本划分成单个单词或标记的函数。
tokenize='spacy'表示使用spaCy工具包中的分词器(tokenizer)进行文本分词。
spaCyPython自然语言处理工具包用于处理文本的各种组件和功能。其分词器可以根据空格、标点符号和其他语言特定的规则来将文本划分成单个标记或单词。
"""
TEXT = data.Field(tokenize='spacy')
"""
用于定义标签数据预处理的类，通常用于分类任务中。其中的dtype参数表示标签的数据类型。
"""
LABEL = data.LabelField(dtype=torch.float)
"""
将IMDB数据集划分为训练集、验证集和测试集，并将文本数据和标签数据与相应的Field对象进行关联。
这样，在后续的处理中，可以直接通过TEXT和LABEL对象来对数据进行预处理、转换和加载。
"""
train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)

print('len of train data:', len(train_data))
print('len of test data:', len(test_data))
print(train_data.examples[15].text)
print(train_data.examples[15].label)

"""
使用训练集数据train_data来构建文本数据的词汇表，并使用预训练的GloVe词向量（100维）来初始化词向量。
预训练的词向量（比如GloVe、Word2Vec等）是在大规模文本数据上训练得到的高质量词向量表示，包含了丰富的语义信息和词汇关系。
可以理解为站在巨人的肩膀上进行分词，也就是直接使用已有的向量库，然后我们再添加新向量到这个已知的向量库中，得到最终的一个词库。
"""
TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')
"""
根据提供的训练集数据train_data构建标签数据的词汇表。对于IMDB数据集，标签只有0和1两种，因此最终生成的词汇表只包含这两个标签。
构建词汇表的过程会为每个标签分配一个唯一的索引，以便在模型训练和评估中使用。
"""
LABEL.build_vocab(train_data)

batchsz = 30
device = torch.device('cuda')
"""
将 train_data 和 test_data 拆分成多个 batches，每个 batch 包含 batchsz 个样本。
在拆分数据时，BucketIterator 会根据数据中每个样本的长度将其放入不同的 bucket 中，并将 bucket 中的样本进行 padding，
以便于以 minibatch 的形式进行训练或测试。BucketIterator 还会在每个 epoch 结束时重新打乱数据，以增加随机性并避免模型过度拟合。
"""
train_iterator, test_iterator = data.BucketIterator.splits(
    (train_data, test_data),
    batch_size = batchsz,
    device = device
)

class RNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(RNN, self).__init__()

        """
        创建一个 embedding layer（嵌入层）。在这里，vocab_size 表示词汇表的大小，即不同单词的数量；
        embedding_dim 表示每个单词将被映射为一个多维向量的维度。Embedding 层可以用来将输入数据中的整数索引（代表单词在词汇表中的位置）转换为密集向量表示，
        这些向量在训练过程中会被学习优化。这种嵌入表示通常用于将离散的词汇信息转换为连续的向量表示，以便神经网络能够更好地处理和学习文本数据。
        """
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        """
        embedding_dim：输入的词嵌入向量的维度。
        hidden_dim：LSTM 单元中隐藏状态的维度。
        num_layers：LSTM 的层数，默认为 2 层。
        bidirectional：是否使用双向 LSTM，即在前向和后向两个方向上进行处理，默认为 True。
        dropout：Dropout 的概率，用于防止过拟合，默认为 0.5。
        """
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.5)
        # [256*2] => [1]
        self.fc = nn.Linear(hidden_dim*2, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        embedding = self.dropout(self.embedding(x))
        output, (hidden, cell) = self.rnn(embedding)
        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
        hidden = self.dropout(hidden)
        out = self.fc(hidden)
        return out

rnn = RNN(len(TEXT.vocab), 100, 256)

pretrained_embedding = TEXT.vocab.vectors
print('pretrained_embedding:', pretrained_embedding.shape)
rnn.embedding.weight.data.copy_(pretrained_embedding)
print('embedding layer inited.')

optimizer = optim.Adam(rnn.parameters(), lr=1e-3)
criteon = nn.BCEWithLogitsLoss().to(device)
rnn.to(device)

import numpy as np

def binary_acc(preds, y):
    preds = torch.round(torch.sigmoid(preds))
    correct = torch.eq(preds, y).float()
    acc = correct.sum() / len(correct)
    return acc

def train(rnn, iterator, optimizer, criteon):
    avg_acc = []
    rnn.train()

    for i, batch in enumerate(iterator):
        pred = rnn(batch.text).squeeze(1)
        loss = criteon(pred, batch.label)
        acc = binary_acc(pred, batch.label).item()
        avg_acc.append(acc)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if i % 10 == 0:
            print(i, acc)

    avg_acc = np.array(avg_acc).mean()
    print('avg acc:', avg_acc)

def eval(rnn, iterator, criteon):
    avg_acc = []
    rnn.eval()

    with torch.no_grad():
        for batch in iterator:
            pred = rnn(batch.text).squeeze(1)
            loss = criteon(pred, batch.label)
            acc = binary_acc(pred, batch.label).item()
            avg_acc.append(acc)

    avg_acc = np.array(avg_acc).mean()
    print('test:', avg_acc)

for epoch in range(10):
    eval(rnn, test_iterator, criteon)
    train(rnn, train_iterator, optimizer, criteon)